# Pod Disruption Budget (PDB) ‚Äì Drain Behavior Explained (Hands-On)

---

## 1. Cluster Setup (Example)

We have a Kubernetes cluster with **1 master and 3 worker nodes**.

```
master-1

worker-1 ‚Üí nginx-pod-1, nginx-pod-2
worker-2 ‚Üí nginx-pod-3, nginx-pod-4
worker-3 ‚Üí nginx-pod-5, nginx-pod-6
```

### Assumptions

* `nginx` is managed by a **Deployment**
* Replica count = **6**
* Pods are evenly distributed by the scheduler

---

## 2. NGINX Deployment (YAML)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

üìå This deployment ensures **6 nginx pods** are always desired.

---

## 3. PodDisruptionBudget Configuration (YAML)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: nginx
```

### Meaning

* At least **4 nginx pods must be AVAILABLE at all times**
* AVAILABLE = `Running + Ready`

---

## 4. Very Important Concepts (Must Understand)

### What PDB Checks

PDB checks **only one thing** during eviction:

> If this pod is evicted now, will AVAILABLE pods fall below `minAvailable`?

### What PDB Does NOT Check

* Node capacity
* Whether other nodes have space
* Whether new pods can be scheduled later
* Whether new nodes can be added

üìå **PDB is enforced at eviction time, not scheduling time**

---

## 5. Hands-On Walkthrough (Step by Step)

### Step 1: Deploy NGINX

```bash
kubectl apply -f nginx-deployment.yaml
```

Verify:

```bash
kubectl get pods -o wide
```

You should see 6 pods distributed across worker nodes.

---

### Step 2: Apply PodDisruptionBudget

```bash
kubectl apply -f nginx-pdb.yaml
```

Verify:

```bash
kubectl get pdb
```

Expected output:

```
NAME        MIN AVAILABLE   ALLOWED DISRUPTIONS
nginx-pdb  4               2
```

---

## 6. Drain Worker-3 (Allowed Case)

### Pods on worker-3

```
nginx-pod-5
nginx-pod-6
```

### Action

```bash
kubectl drain worker-3 --ignore-daemonsets
```

### Availability Calculation

| State                 | Available Pods |
| --------------------- | -------------- |
| Before eviction       | 6              |
| After evicting 2 pods | 4              |

‚úÖ PDB condition satisfied (`4 ‚â• minAvailable 4`)

### Result

Pods are recreated on other nodes:

```
worker-1 ‚Üí nginx-pod-1, nginx-pod-2, nginx-pod-5
worker-2 ‚Üí nginx-pod-3, nginx-pod-4, nginx-pod-6
worker-3 ‚Üí drained
```

Drain **succeeds**.

---

## 7. Drain Worker-2 (Blocked Case)

### Pods on worker-2

```
nginx-pod-3
nginx-pod-4
nginx-pod-6
```

### Drain Command

```bash
kubectl drain worker-2 --ignore-daemonsets
```

### Eviction Happens One Pod at a Time

| Eviction Step | Available Pods | Allowed              |
| ------------- | -------------- | -------------------- |
| Evict pod 1   | 6 ‚Üí 5          | ‚úÖ Yes                |
| Evict pod 2   | 5 ‚Üí 4          | ‚úÖ Yes                |
| Evict pod 3   | 4 ‚Üí 3          | ‚ùå No (PDB violation) |

üö® Drain stops immediately.

---

## 8. Revert / Backup Action (Uncordon Node)

If you want to **revert the drain operation** and allow the node to schedule pods again:

```bash
kubectl uncordon worker-2
```

### What this does

* Marks the node as **Schedulable**
* New pods can again be placed on `worker-2`
* Does **not** automatically move existing pods back

üìå `uncordon` only re-enables scheduling, it does not rebalance pods.

---

## 9. Common Misunderstanding (Very Important)

‚ùå Wrong assumption:

> Drain failed because pods cannot be scheduled on other nodes

‚úÖ Correct reason:

> Drain failed because evicting another pod would reduce AVAILABLE pods below `minAvailable`

üìå Scheduler is **not involved yet**.

---

## 10. Maximum Allowed Disruption

```
Total pods = 6
minAvailable = 4

Allowed disruptions = 6 - 4 = 2
```

‚û°Ô∏è Only **2 nginx pods** can be disrupted at a time.

---

## 11. How to Successfully Drain Worker-2

### Option 1: Scale Deployment (YAML)

```yaml
spec:
  replicas: 7
```

Then:

```
Allowed disruptions = 7 - 4 = 3
```

---

### Option 2: Relax the PDB

```yaml
spec:
  minAvailable: 3
```

---

### Option 3: Add a New Worker Node

‚ö†Ô∏è Important:

> Pods must already be **Running & Ready** on other nodes **before eviction starts**

---

## 12. When Do We Use PodDisruptionBudget (PDB)?

PDB is used whenever **planned disruptions** can affect application availability.

### Common Real-World Scenarios

### 1. Node Maintenance

* OS patching
* Kernel upgrades
* Hardware replacement
* Cloud provider maintenance

‚û°Ô∏è Ensures minimum pods stay available during node drain.

---

### 2. Cluster Scaling

* Scaling worker nodes down
* Auto-scaling events

‚û°Ô∏è Prevents too many pods from being evicted at once.

---

### 3. Application Upgrades

* Rolling updates
* Configuration changes
* Image upgrades

‚û°Ô∏è Guarantees service availability during updates.

---

### 4. Preventing Human Errors

* Accidental `kubectl drain`
* Maintenance during peak traffic

‚û°Ô∏è Acts as a **safety guardrail**.

---

### 5. Stateful or Critical Services

* Databases
* Authentication services
* Payment services

‚û°Ô∏è Ensures quorum or minimum replicas are always up.

---

## 13. Key Takeaway

**PodDisruptionBudget protects application availability, not node maintenance convenience.**

Eviction safety is checked first. Scheduling happens later.

---


